{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis of IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part 2 of a series of projects to help me understand the real-life applications of Machine Learning. I will be trying to answer the question, \"How do startup fintech companies provide sentiment based trading signals to investment professionals?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having worked within investment management for many years, I have become interested in understanding the real-life applications of Machine Learning, specifically Natural Language Processing (NLP), within the finance industry. There is a huge amount of information that a Chartered Financial Analyst needs to navigate through. Traditional methods of analysis needs to be augmented with state-of-the-art data science techniques. \n",
    "\n",
    "Some of the really exciting potential applications include:\n",
    "-  Trading signals derived from Sentiment Analysis\n",
    "-  Classification and clustering of financial related documents.\n",
    "-  Auto summarisation of text documents including transcipts of company conference calls. \n",
    "-  Processing company earnings reports in the quickest time possible in order to gain an information advantage.\n",
    "\n",
    "My plan is to conduct a series of small projects, narrow in scope, that will allow me to keep within my abilities. From my initial research, I become inspired by the \"institutional quality data feeds\" built into the Quantopian.com pipeline. For example: \n",
    "-  StockTwits Trade Mood from PsychSignal\n",
    "-  Twitter Trader Mood from PsychSignal\n",
    "-  Sentdex Sentiment Analysis\n",
    "\n",
    "(I have not developed any trading algorithms on Quantopian yet because my focus is on becoming a Data Scientist.)\n",
    "\n",
    "**How do startup companies provide sentiment based trading signals to investment professionals?**\n",
    "\n",
    "To tackle this question, I will conduct several loosely related project workstreams: \n",
    "-  News classification - filtering general news articles to those related to 'Business'\n",
    "-  Sentiment Analysis (postive or negative) - movie reviews and Twitter feeds. \n",
    "-  Applying Daily News to to predict Stock Market returns. \n",
    "\n",
    "These building blocks will be powerful components that I can adapt as a framework for use in future work related to Sentiment Analysis and NLP. \n",
    "\n",
    "One burning question you might want me to answer immediately is whether I have found any Alpha? Yes. To a certain extent. At this stage of my analysis there is not enough Alpha on a stand-alone basis for a complete trading model. But I am very confident that with more work, there is enough Alpha there to be extracted and used as a part of an overall strategy, namely the Sentiment-based component. \n",
    "\n",
    "I am constantly learning and there is always room for improvements. \n",
    "\n",
    "Please note that I am only able to use non-proprietary, publicly available datasets, which will obviously lessen the information advantage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset of movie reviews from the IMDb (Internet Movie Database) website collected by Stanford researcher Andrew Maas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The dataset is available at <http://ai.stanford.edu/~amaas/data/sentiment/>\n",
    "-  The original pubilication Andrew L. Maas, et al (2011) [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains the text of the reviews, together with a label that indicates whether a review is “positive” or “negative.” The IMDb website itself contains ratings from 1 to 10. To simplify the modeling, this annotation is summarized as a two-class classification dataset where reviews with a score of 7 or higher are labeled as positive, and the rest as negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided as individual txt files within 2 folders:\n",
    "-  Train data: 25,000 labelled text files\n",
    "-  Test data: 25,000 labelled text files\n",
    "\n",
    "The train and test folders, each contain two sub‐folders called pos and neg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1a Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "%time reviews_train = load_files(\"C:/Python Project Files/aclImdb/train\")\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text_train: 25000\n",
      "text_train[20]:\n",
      "b\"This independent, B&W, DV feature consistently shocks, amazes and amuses with it's ability to create the most insane situations and then find humor and interest in them. It's all hilarious and ridiculous stuff, yet as absurd as much of the film should be, there is a heart and a reality here that keeps the film grounded, keeps the entire piece from drifting into complete craziness and therein lies the real message here. This film is about how we all survive in a world gone mad. That seems to be the heart of the film. For as insane and off the wall as things get, Leon, the 30 yr. old paperboy-protagonist, always tries to keep it together. He's like a child forever trying to catch the balloon that is floating away so that everything will work out for the best, so that everyone can have what they want.<br /><br />The acting in the film could have went far over the top but the exceptional cast really keeps the piece cohesive. Van Meter is perhaps the best of the bunch here with a performance that shines through her absurd diseased tics. Just as the characters in the film do, we overlook her sudden outbursts to see the real person underneath. <br /><br />Majkowski is a true genius here. He takes the utmost ridiculous plot twists and keeps them real. It is his script and his cast that help keep the whole thing afloat. It's a true testament to the skill of Majkowski and all involved that this film, with it's grating plot and characters, never once works our nerves. Majkowski has taken a film that could have been abrasive and repugnant, and somehow given it heart and humor. This is a unique film. Not to be missed. <br /><br />\"\n"
     ]
    }
   ],
   "source": [
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[20]:\\n{}\".format(text_train[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data and remove HTML line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.53 s\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "%time reviews_test = load_files(\"C:/Python Project Files/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Simple Bag of Words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most simple but effective ways to represent text is by using the Bag of Words method. Here the model learns a vocabulary from all of the documents by discarding most of the structure of the input test, and only counting the number of times each word appears in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer is used in the tokenization of the training data and building of the volcabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.95 s\n",
      "Wall time: 8.06 s\n",
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%time vect = CountVectorizer().fit(text_train)\n",
    "%time X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "Features 8000 to 8020:\n",
      "['blustering', 'blusters', 'blustery', 'blut', 'bluth', 'bluto', 'blvd', 'blystone', 'blyth', 'blythe', 'blythen', 'blyton', 'bmacv', 'bmi', 'bmob', 'bmoc', 'bmoviefreak', 'bmovies', 'bmw', 'bmws']\n",
      "Every 2000th feature:\n",
      "['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Features 8000 to 8020:\\n{}\".format(feature_names[8000:8020]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic Regression using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Logistic regression will be carried out using k-fold cross validation.\n",
    "\n",
    "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88136\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**88.1% accuracy using 5-fold cross validation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tokens at 74,849 is too high. To improve the extraction of words we should cut down this number and and only use tokens that appears in at least 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27,272 features is about a third of the original features. Better but still quite high. \n",
    "\n",
    "Let's take a look at these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 27271\n",
      "Features 8000 to 8020:\n",
      "['elton', 'elude', 'eluded', 'eludes', 'elusive', 'elves', 'elvira', 'elvis', 'ely', 'em', 'email', 'emails', 'emanating', 'emancipation', 'emanuelle', 'emasculated', 'embarassing', 'embark', 'embarking', 'embarks']\n",
      "Every 2000th feature:\n",
      "['00', 'baked', 'centipede', 'cutlery', 'elton', 'gaining', 'ideals', 'leering', 'moxy', 'picasso', 'repartee', 'silvers', 'talkative', 'verisimilitude']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Features 8000 to 8020:\\n{}\".format(feature_names[8000:8020]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove words that appear too frequently to be informative. \n",
    "scikit-learn has a built-in list of English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['towards', 'take', 'bottom', 'him', 'moreover', 'via', 'sixty', 'no', 'itself', 'often', 'with', 'seem', 'get', 'a', 'fire', 'is', 'within', 'de', 'whatever', 'amount', 'over', 'see', 'everyone', 'has', 'beyond', 'due', 'throughout', 'keep', 'somewhere', 'below', 'whom', 'in']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing 318 stopwords is unlikely to have much impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Specifying stop_words=\"english\" uses the built-in list.\n",
    "# We could also augment it and pass our own.\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Applying tf–idf to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency–inverse document frequency (tf–idf) gives high weight to any term that appears\n",
    "often in a particular document, but not in many documents in the corpus. These terms are likely\n",
    "to be very descriptive of the content of that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent information leak when applying tf-idf, we use a pipeline to glue multiple processing steps together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4a Using pipeline and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:\n",
      "{'logisticregression__C': 0.001}\n",
      "Wall time: 10min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n",
    "                     LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**89% accuracy using 5-fold cross validation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some improvement using tf-idf but not much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features with the highest tf-idf identify specific films such as Titanic and Bridget Jones Diary. These words are unlikely to help in our sentiment classification task. \n",
    "\n",
    "Also words that appear frequently and deemed less important are actually very useful in movie sentiment analysis, such as 'movie', 'film', 'good', 'great', and 'bad'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bag-of-Words with More Than One Word (n-Grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main disadvantages of using a bag-of-words representation is that word\n",
    "order is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n",
    "“it’s good, not bad at all” have exactly the same representation, even though the meanings are inverted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\n",
    "more generally sequences of tokens are known as n-grams: CountVectorizer(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.91\n",
      "Best parameters:\n",
      "{'logisticregression__C': 100, 'tfidfvectorizer__ngram_range': (1, 3)}\n",
      "Wall time: 1h 23min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "# running the grid search takes a long time because of the\n",
    "# relatively large grid and the inclusion of trigrams\n",
    "param_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "\"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "model = grid.fit(text_train, y_train)\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**91% accuracy using 5-fold cross validation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Lessons I have learnt from this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New tools to add to skillset:\n",
    "-  Using the load_files function from sklearn to handle 50,000 individual text files\n",
    "-  CountVectorizer(min_df=5)\n",
    "-  Using pipeline\n",
    "-  Using GridSearchCV\n",
    "-  tf-idf\n",
    "-  n-Grams (1, 1), (1, 2), (1, 3)]\n",
    "\n",
    "Practiced:\n",
    "-  Logistic Regression\n",
    "-  Bag of words\n",
    "-  tf-idf\n",
    "-  Removing stopwords\n",
    "-  sklearn features extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
